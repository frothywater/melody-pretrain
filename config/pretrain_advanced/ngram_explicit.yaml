# lightning.pytorch==1.8.3.post1
seed_everything: 42
trainer:
  default_root_dir: experiment/model/pretrain_ngram_explicit
  max_steps: 5000
  accelerator: gpu
  strategy: deepspeed_stage_2
  precision: 16
  accumulate_grad_batches: 4
  gradient_clip_val: 1.0
  log_every_n_steps: 1
  callbacks:
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: ModelCheckpoint
      init_args:
        every_n_train_steps: 1000
        save_on_train_epoch_end: true
        filename: "{step}.ckpt"
model:
  dataset_dir: experiment/dataset/pretrain_base
  ngram_classification: true
data:
  batch_size: 64
  num_workers: 4
  dataset_dir: experiment/dataset/pretrain_base
  load_ngram_data: true
  pitch_augumentation: true
  data_collator:
    class_path: DataCollatorForPrefixMaskedLanguageModeling
    init_args:
      seq_len: 512
      random_crop: true
      ngram_classification: true
      ngram_field_specific_masking: true
      masking:
        class_path: MultiTargetInfillingMasking
        init_args:
          probabilities: [0.25, 0.25, 0.5]
          maskings:
            - class_path: RandomNgramMasking
              init_args:
                corruption_rate: 0.25
                extra_data_field_name: pitch_ngrams
            - class_path: RandomNgramMasking
              init_args:
                corruption_rate: 0.25
                extra_data_field_name: rhythm_ngrams
            - class_path: SingleSpanMasking
              init_args:
                corruption_rate: 0.5